description: "TDS Virtual TA Project Sample (but not the actual evaluation) Questions"

providers:
  # 1) your FastAPI endpoint
  - id: local
    config:
      url: "http://127.0.0.1:8000/query"
      method: POST
      headers:
        Content-Type: "application/json"
      body:
        question: "{{ prompt }}"
        image: "{{ image | default(null) }}"
      transformResponse: json

  # 2) your OpenAI proxy, used only for rubric‐style grading
  - id: aiproxy
    config:
      url: "https://aiproxy.sanand.workers.dev/openai/v1/chat/completions"
      method: POST
      headers:
        Content-Type: "application/json"
        Authorization: "Bearer {{ env.API_KEY }}"
      body:
        model: "gpt-4o-mini"
        messages:
          - role: system
            content: >-
              You are an evaluator that checks if an output meets specific criteria.
              Analyze the output based on the given rubric and respond with a JSON
              object containing {"reason":"your analysis","score": number between
              0.0 and 1.0,"pass":true/false}.
          - role: user
            content: >-
              Output to evaluate: {{ output }}

              Rubric: {{ rubric }}
        temperature: 0
      transformResponse: json

# By default run all tests against your FastAPI (/query) via the "local" provider
defaultTest:
  options:
    provider:
      id: local

tests:
  - name: clarify-model-use
    vars:
      prompt: >-
        The question asks to use gpt-3.5-turbo-0125 model but the ai-proxy
        provided by Anand sir only supports gpt-4o-mini. So should we just use
        gpt-4o-mini or use the OpenAI API for gpt3.5 turbo?
      image: project-tds-virtual-ta-q1.webp
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Clarifies use of gpt-3.5-turbo-0125 not gpt-4o-mini
      - type: contains
        transform: JSON.stringify(output.links)
        value: https://discourse.onlinedegree.iitm.ac.in/t/ga5-question-8-clarification/155939

  - name: ga4-bonus-dashboard
    vars:
      prompt: >-
        If a student scores 10/10 on GA4 as well as a bonus, how would it appear
        on the dashboard?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Mentions the dashboard showing "110"
      - type: contains
        transform: JSON.stringify(output.links)
        value: https://discourse.onlinedegree.iitm.ac.in/t/ga4-data-sourcing-discussion-thread-tds-jan-2025/165959

  - name: docker-vs-podman
    vars:
      prompt: >-
        I know Docker but have not used Podman before. Should I use Docker for
        this course?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Recommends Podman for the course
      - type: llm-rubric
        transform: output.answer
        value: Mentions that Docker is acceptable
      - type: contains
        transform: JSON.stringify(output.links)
        value: https://tds.s-anand.net/#/docker

  - name: tds-exam-date
    vars:
      prompt: When is the TDS Sep 2025 end-term exam?
    assert:
      - type: llm-rubric
        transform: output.answer
        value: Says it doesn't know

  # …continue adding the rest of your tests in this same pattern…

# Anything under `evaluations:` runs against the aiproxy provider
evaluations:
  - name: rubric-evaluation
    options:
      provider:
        id: aiproxy
    assert:
      - type: is-json
        value:
          type: object
          required: [reason, score, pass]
          properties:
            reason:
              type: string
            score:
              type: number
            pass:
              type: boolean

writeLatestResults: true
commandLineOptions:
  cache: false